{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project web scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as Bsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ITA_weather_downloader:\n",
    "    '''  This downloader scrapes ilmeteo.it website. It downloads monthly tables with daily weather data in\n",
    "         Italian cities for which data for each month in the chosen interval are available at the website.   '''\n",
    "    def ___init___(self, start_year=1985, end_year=2019, partitions=2):\n",
    "        \n",
    "        ''' At the initialization of the object we can choose the start and end years. Data from January to December \n",
    "        of each year between the start and end year (included) will be then downloaded. The default values which we\n",
    "        use in our project are 1985 and 2019, respectively.\n",
    "        \n",
    "        In addition option \"partitions\" gives into how many parts are the final links partitioned (with respect\n",
    "        to the chosen cities). This will enable user to download data by parts in order to be more time efficient \n",
    "        or if they are satisfied with only incomplete data.\n",
    "        \n",
    "        Calling ITA_weather_downloader class automatically imports requests and BeautifulSoup.'''\n",
    "        \n",
    "        self.start_year=start_year\n",
    "        self.end_year=end_year\n",
    "        self.partitions=partitions\n",
    "        self.basic_link=\"https://www.ilmeteo.it/portale/\"\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup as Bsoup\n",
    "    def __repr__(self):\n",
    "        return f\"Italian weather data for interval {self.start_year} - {self.end_year}.\"\n",
    "       \n",
    "    \n",
    "    def region_pages(self):\n",
    "        '''   This method finds links to the pages of the regions and their names.    '''\n",
    "        self.link2=\"https://www.ilmeteo.it/portale/archivio-meteo/\"\n",
    "        beginning = requests.get(self.link2) \n",
    "        soup_beginning=Bsoup(beginning.text)\n",
    "        horalka=soup_beginning.findAll(\"td\")\n",
    "        rumba=horalka[1].findAll(\"a\")\n",
    "        reg_partial_links=[x.get(\"href\") for x in rumba]\n",
    "        self.reg_links=[self.basic_link+region for region in reg_partial_links]   # complete links\n",
    "        region_names=[x.text for x in rumba]                       # list of all regions\n",
    "        self.region_names=region_names\n",
    "        return self.reg_links\n",
    "    \n",
    "    def city_pages(self, regions=region_names):\n",
    "        '''  This method finds links to the data pages for the cities and their names.\n",
    "        List of names of the regions to which search should be limited can be provided. The default option is all regions. '''\n",
    "        cities=[]\n",
    "        cities_partial_links=[]\n",
    "        indeces=[self.region_names.index(x) for x in regions]\n",
    "        chosen_reg_links=[self.reg_link[x] for x in indeces]\n",
    "        for reg_link in chosen_reg_links:\n",
    "            reg_page = requests.get(reg_link)\n",
    "            soup_reg_page=Bsoup(reg_page.text)\n",
    "            tea=soup_reg_page.findAll(\"div\",{\"class\":\"block noborder\"})\n",
    "            cofee=tea[0].findAll(\"a\",{\"target\":\"\"})\n",
    "            for x in cofee:\n",
    "                cities_partial_links.append(x.get(\"href\"))   # links\n",
    "                cities.append(x.text)                  # city names\n",
    "        self.cities=sorted(list(set(cities)))    # to get rid of the reccurent cities, setting alphabetical order\n",
    "        cities_partial_links=sorted(list(set(cities_partial_links)))   # to get rid of the reccurent cities, setting alphabetical order\n",
    "        self.cities_links=[self.basic_link+city_link for city_link in cities_partial_links]\n",
    "        return self.cities_links\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 \n",
    "## Getting links to the webpages for the chosen cities' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  basic links   '''\n",
    "link1=\"https://www.ilmeteo.it/portale/\"\n",
    "link2=\"https://www.ilmeteo.it/portale/archivio-meteo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   Finding links to the pages of the regions    '''\n",
    "beginning = requests.get(link2) \n",
    "soup_beginning=Bsoup(beginning.text)\n",
    "horalka=soup_beginning.findAll(\"td\")\n",
    "rumba=horalka[1].findAll(\"a\")\n",
    "reg_partial_links=[]\n",
    "for x in rumba:\n",
    "    reg_partial_links.append(x.get(\"href\"))\n",
    "\n",
    "reg_links=[]\n",
    "for region in reg_partial_links:\n",
    "    reg_links.append(link1+region)\n",
    "    \n",
    "'''  Region names  '''\n",
    "region_names=[]\n",
    "for x in rumba:\n",
    "    region_names.append(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   Gerting links to the data pages for the cities  '''\n",
    "cities=[]\n",
    "cities_partial_links=[]\n",
    "for reg_link in reg_links:\n",
    "    reg_page = requests.get(reg_link) \n",
    "    soup_reg_page=Bsoup(reg_page.text)\n",
    "    tea=soup_reg_page.findAll(\"div\",{\"class\":\"block noborder\"})\n",
    "    cofee=tea[0].findAll(\"a\",{\"target\":\"\"})\n",
    "    for x in cofee:\n",
    "        cities_partial_links.append(x.get(\"href\"))   # links\n",
    "        cities.append(x.text)                  # city names\n",
    "        \n",
    "cities=sorted(list(set(cities)))    # to get rid of the reccurent cities, setting alphabetical order\n",
    "cities_partial_links=sorted(list(set(cities_partial_links)))   # to get rid of the reccurent cities, setting alphabetical order\n",
    "\n",
    "cities_links=[]\n",
    "for city_link in cities_partial_links:\n",
    "    cities_links.append(link1+city_link)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: for some reason some of the links lead to a no-data page (links are correct, we get the same result if we move directly on the Italian webpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering of the cities which have data available for the required time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  Getting links to the monthly data from the pages for the cities   '''\n",
    "city_years=[]\n",
    "city_months=[]\n",
    "MONTH_links=[]\n",
    "for city in cities_links:\n",
    "    YEARS=[]\n",
    "    MONTHS=[]\n",
    "    page = requests.get(city)\n",
    "    soup1= Bsoup(page.text)\n",
    "    temps_dark = soup1.findAll(\"tr\",{'class':'dark'})\n",
    "    temps_light = soup1.findAll(\"tr\",{'class':'light'})\n",
    "    for x in temps_dark:\n",
    "        tds=x.findAll(\"td\")\n",
    "        YEARS.append(tds[0].text)                              \n",
    "        links=[y.get(\"href\") for y in tds[1].findAll(\"a\")]\n",
    "        months=[y.text for y in tds[1].findAll(\"a\")]\n",
    "        MONTH_links.append(links)\n",
    "        MONTHS.append({tds[0].text:months})\n",
    "    for x in temps_light:\n",
    "        tds=x.findAll(\"td\")\n",
    "        YEARS.append(tds[0].text)\n",
    "        links=[y.get(\"href\") for y in tds[1].findAll(\"a\")]\n",
    "        months=[y.text for y in tds[1].findAll(\"a\")]\n",
    "        MONTH_links.append(links)\n",
    "        MONTHS.append({tds[0].text:months})\n",
    "    city_years.append(sorted(YEARS))    # list of lists of years for each city during which at least some data were recorded\n",
    "    city_months.append([y for x,y in sorted([(list(x.keys()),x) for x in MONTHS])])  # list of months from which data are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting links to the webpages with the data tables that we want  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''     Making working links to the available monthly data-tables     ''' \n",
    "final_links=[]\n",
    "for city in MONTH_links:\n",
    "    for monthly_link in city:\n",
    "        final_links.append(link1+monthly_link)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    Making sorted lists of cities and to them corresponding lists of years and months \n",
    "                                              for which at least some data are available     '''\n",
    "\n",
    "good_cities=[]\n",
    "good_months=[]\n",
    "good_years=[]\n",
    "for i in range(len(cities)):\n",
    "    if len(city_years[i])!=0:\n",
    "        good_cities.append(cities[i])\n",
    "        good_months.append(city_months[i])\n",
    "        good_years.append(city_years[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altogether we have 40,730 available links. We will subset links for the cities which have available data for each month during the 1985-2019 period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Subsetting only cities which have data available for every month during the chosen period  '''\n",
    "#month_list=[y for z in good_months for y in z]\n",
    "\n",
    "start_year=1985\n",
    "a=start_year\n",
    "end_year=2019\n",
    "chosen_years=[a]\n",
    "while a<end_year:                # getting the list of chosen years\n",
    "    chosen_years.append(a+1)\n",
    "    a+=1                       \n",
    "\n",
    "chosen_cities=[]\n",
    "for city in good_cities:          # getting the list of cities for which all months in the chosen years are provided\n",
    "    adidas=[]\n",
    "    for year in chosen_years:\n",
    "        string=city+\"/\"+str(year)+\"/\"+\"Gennaio\"\n",
    "        indeces=[MONTH_links.index(y) for y in MONTH_links for x in y if string in x] #is link for the January of the chosen\n",
    "        if len(indeces)!=0:                                                              # year in the list?\n",
    "            index=indeces[0]\n",
    "            if len(MONTH_links[index])==12:\n",
    "                adidas.append(1)\n",
    "    if sum(adidas)==len(chosen_years):\n",
    "        chosen_cities.append(city)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We get 52 cities. For 35 years we would have 21,840 monthly data tables. It seems that half of it would be quite enough, therefore we will take only every second city in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_cities1=chosen_cities[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  Choosing final links for the chosen cities and years    ''' \n",
    "chosen_links=[]\n",
    "for link in final_links:\n",
    "    for city in chosen_cities1:\n",
    "        for year in chosen_years:\n",
    "            if city in link and str(year) in link:\n",
    "                chosen_links.append(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Making lists of cities, years and months corresponding to the list of the chosen links  '''   \n",
    "final_cities=[x.split(\"/\")[5] for x in chosen_links]\n",
    "final_years=[x.split(\"/\")[6] for x in chosen_links]\n",
    "final_months=[x.split(\"/\")[7] for x in chosen_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So in the end we get 10,920 links to each month in the 1985-2019 period for 26 Italian cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2\n",
    "## Scraping of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''       Here we have the algorithm for getting all of the data and variable names from the linked data-pages    '''\n",
    "table=[]\n",
    "for link in chosen_links:\n",
    "    try:\n",
    "        index=chosen_links.index(link)\n",
    "        month=final_months[index]    # what month do we scrape?\n",
    "        year=str(final_years[index])      # what year do we scrape?\n",
    "        city=final_cities[index]    # what city do we scrape?\n",
    "        Dark_values=[]\n",
    "        Light_values=[]\n",
    "        values=[]\n",
    "        page = requests.get(link)             # getting linked webpage\n",
    "        soup1= Bsoup(page.text)\n",
    "        var=soup1.findAll(\"table\")[3].findAll(\"th\")    # find the list of variable names on the webpage\n",
    "        variables=[\"city\",\"year\",\"month\"]                    # making the list of variable names...\n",
    "        [variables.append(x.text) for x in var]     #  ... (for each link separately to check if they are always the same)\n",
    "        temps_dark = soup1.findAll(\"tr\",{'class':'dark'})    # data are in a table with alternating dark \n",
    "        temps_light = soup1.findAll(\"tr\",{'class':'light'})   # and light rows\n",
    "        for x in temps_dark:\n",
    "            tds=x.findAll(\"td\")\n",
    "            dark_value=[city,year,month]\n",
    "            [dark_value.append(y.text) for y in tds]            # getting list of values from each dark row\n",
    "            Dark_values.append(dark_value)\n",
    "        for x in temps_light:\n",
    "            tds=x.findAll(\"td\")\n",
    "            light_value=[city,year,month]\n",
    "            [light_value.append(y.text) for y in tds]          # getting list of values from each light row\n",
    "            Light_values.append(light_value)\n",
    "        if len(temps_dark)==(len(temps_light)+1) or len(temps_dark)==len(temps_light):\n",
    "            for i in range(len(temps_light)):                              \n",
    "                values.append(Dark_values[i])                   # list of values for months with even number of days\n",
    "                values.append(Light_values[i])                         \n",
    "        else:\n",
    "            for i in range(len(temps_light)):\n",
    "                values.append(Dark_values[i])                   # list of values for months with odd number of days\n",
    "                values.append(Light_values[i])\n",
    "            values.append(Dark_values[len(temps_light)])\n",
    "        tab={variables[i]:[x[i] for x in values] for i in range(len(variables))} # creating a monthly dictionary - variable:list_of_values\n",
    "        table.append(tab)               # in the end, we get a list of monthly dictionaries\n",
    "        if index%200==0:     # checking the progress\n",
    "            print(index)\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(current_time)        \n",
    "    except:\n",
    "        print(chosen_links.index(link))     # if errors, where?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There were some errors for 988th, 10,751th  and 10,775th links, so let's add them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_links=[chosen_links[988],chosen_links[10751],chosen_links[10775]]\n",
    "xtable=[]\n",
    "for link in add_links:\n",
    "    try:\n",
    "        index=chosen_links.index(link)\n",
    "        month=final_months[index]    # what month do we scrape?\n",
    "        year=str(final_years[index])      # what year do we scrape?\n",
    "        city=final_cities[index]    # what city do we scrape?\n",
    "        Dark_values=[]\n",
    "        Light_values=[]\n",
    "        values=[]\n",
    "        page = requests.get(link)             # getting linked webpage\n",
    "        soup1= Bsoup(page.text)\n",
    "        var=soup1.findAll(\"table\")[3].findAll(\"th\")    # find the list of variable names on the webpage\n",
    "        variables=[\"city\",\"year\",\"month\"]                    # making the list of variable names...\n",
    "        [variables.append(x.text) for x in var]     #  ... (for each link separately to check if they are always the same)\n",
    "        temps_dark = soup1.findAll(\"tr\",{'class':'dark'})    # data are in a table with alternating dark \n",
    "        temps_light = soup1.findAll(\"tr\",{'class':'light'})   # and light rows\n",
    "        for x in temps_dark:\n",
    "            tds=x.findAll(\"td\")\n",
    "            dark_value=[city,year,month]\n",
    "            [dark_value.append(y.text) for y in tds]            # getting list of values from each dark row\n",
    "            Dark_values.append(dark_value)\n",
    "        for x in temps_light:\n",
    "            tds=x.findAll(\"td\")\n",
    "            light_value=[city,year,month]\n",
    "            [light_value.append(y.text) for y in tds]          # getting list of values from each light row\n",
    "            Light_values.append(light_value)\n",
    "        if len(temps_dark)==(len(temps_light)+1) or len(temps_dark)==len(temps_light):\n",
    "            for i in range(len(temps_light)):                              \n",
    "                values.append(Dark_values[i])                   # list of values for months with even number of days\n",
    "                values.append(Light_values[i])                         \n",
    "        else:\n",
    "            for i in range(len(temps_light)):\n",
    "                values.append(Dark_values[i])                   # list of values for months with odd number of days\n",
    "                values.append(Light_values[i])\n",
    "            values.append(Dark_values[len(temps_light)])\n",
    "        tab={variables[i]:[x[i] for x in values] for i in range(len(variables))} # creating a monthly dictionary - variable:list_of_values\n",
    "        xtable.append(tab)               # in the end, we get a list of monthly dictionaries\n",
    "    except:\n",
    "        print(chosen_links.index(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   Adding the missing table, not to be rerun    '''\n",
    "#table.insert(988,xtable[0])     \n",
    "#table.insert(10751,xtable[1])\n",
    "#table.insert(10775,xtable[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  Are the name of the variables always the same (including their order)?    '''\n",
    "sum([list(x.keys())==list(table[0].keys()) for x in table])==len(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    Creating list of dataframes - each month is a single dataframe  '''\n",
    "list_of_DFs=[pd.DataFrame(x) for x in table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''     Exporting dataframes to the csv files        '''\n",
    "df_paths=['d:\\moje_dokumenty\\Desktop\\IES\\semester 11\\Python\\project\\data\\ '+str(chosen_links.index(x))+\".csv\" for x in chosen_links]\n",
    "[list_of_DFs[df_paths.index(x)].to_csv(x, index = None, header=True) for x in df_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding data from the second half of the cities which have well-reported data.\n",
    "### For the time consumption reasons, we took data only from half of the cities. We can add the second half separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  Repeat the same for the other half of the cities '''\n",
    "chosen_cities2=chosen_cities[1::2]\n",
    "'''  Choosing final links for the chosen cities and years    ''' \n",
    "chosen_links2=[]\n",
    "for link in final_links:\n",
    "    for city in chosen_cities2:\n",
    "        for year in chosen_years:\n",
    "            if city in link and str(year) in link:\n",
    "                chosen_links2.append(link)\n",
    "''' Making lists of cities, years and months corresponding to the list of the second set of chosen links  '''   \n",
    "final_cities2=[x.split(\"/\")[5] for x in chosen_links2]\n",
    "final_years2=[x.split(\"/\")[6] for x in chosen_links2]\n",
    "final_months2=[x.split(\"/\")[7] for x in chosen_links2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "14:32:59\n",
      "200\n",
      "14:35:43\n",
      "400\n",
      "14:38:18\n",
      "600\n",
      "14:40:53\n",
      "800\n",
      "14:43:31\n",
      "1000\n",
      "14:46:01\n",
      "1200\n",
      "14:48:29\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1800\n",
      "14:50:41\n",
      "2000\n",
      "14:53:23\n",
      "2200\n",
      "14:55:50\n",
      "2400\n",
      "14:58:17\n",
      "2600\n",
      "15:00:39\n",
      "2800\n",
      "15:03:14\n",
      "3000\n",
      "15:05:38\n",
      "3200\n",
      "15:08:02\n",
      "3400\n",
      "15:10:18\n",
      "3600\n",
      "15:12:45\n",
      "3800\n",
      "15:15:14\n",
      "4000\n",
      "15:17:43\n",
      "4200\n",
      "15:20:06\n",
      "4400\n",
      "15:22:31\n",
      "4600\n",
      "15:24:54\n",
      "4800\n",
      "15:27:20\n",
      "5000\n",
      "15:29:43\n",
      "5200\n",
      "15:32:22\n",
      "5400\n",
      "15:34:44\n",
      "5600\n",
      "15:37:09\n",
      "5800\n",
      "15:39:34\n",
      "6000\n",
      "15:41:56\n",
      "6200\n",
      "15:44:23\n",
      "6400\n",
      "15:46:48\n",
      "6600\n",
      "15:49:16\n",
      "6800\n",
      "15:51:41\n",
      "7000\n",
      "15:54:06\n",
      "7200\n",
      "15:56:22\n",
      "7400\n",
      "15:58:38\n",
      "7600\n",
      "16:00:56\n",
      "7800\n",
      "16:03:18\n",
      "8000\n",
      "16:05:40\n",
      "8200\n",
      "16:08:07\n",
      "8400\n",
      "16:10:30\n",
      "8600\n",
      "16:12:56\n",
      "8800\n",
      "16:15:18\n",
      "9000\n",
      "16:17:38\n",
      "9200\n",
      "16:19:57\n",
      "9400\n",
      "16:22:15\n",
      "9600\n",
      "16:24:28\n",
      "9800\n",
      "16:26:51\n",
      "10000\n",
      "16:29:12\n",
      "10200\n",
      "16:31:37\n",
      "10400\n",
      "16:34:02\n",
      "10600\n",
      "16:36:21\n",
      "10800\n",
      "16:38:44\n"
     ]
    }
   ],
   "source": [
    "'''       Here we have the algorithm for getting all of the data and variable names from the linked data-pages    '''\n",
    "table2=[]\n",
    "for link in chosen_links2:\n",
    "    try:\n",
    "        index=chosen_links2.index(link)\n",
    "        month=final_months2[index]    # what month do we scrape?\n",
    "        year=str(final_years2[index])      # what year do we scrape?\n",
    "        city=final_cities2[index]    # what city do we scrape?\n",
    "        Dark_values=[]\n",
    "        Light_values=[]\n",
    "        values=[]\n",
    "        page = requests.get(link)             # getting linked webpage\n",
    "        soup1= Bsoup(page.text)\n",
    "        var=soup1.findAll(\"table\")[3].findAll(\"th\")    # find the list of variable names on the webpage\n",
    "        variables=[\"city\",\"year\",\"month\"]                    # making the list of variable names...\n",
    "        [variables.append(x.text) for x in var]     #  ... (for each link separately to check if they are always the same)\n",
    "        temps_dark = soup1.findAll(\"tr\",{'class':'dark'})    # data are in a table with alternating dark \n",
    "        temps_light = soup1.findAll(\"tr\",{'class':'light'})   # and light rows\n",
    "        for x in temps_dark:\n",
    "            tds=x.findAll(\"td\")\n",
    "            dark_value=[city,year,month]\n",
    "            [dark_value.append(y.text) for y in tds]            # getting list of values from each dark row\n",
    "            Dark_values.append(dark_value)\n",
    "        for x in temps_light:\n",
    "            tds=x.findAll(\"td\")\n",
    "            light_value=[city,year,month]\n",
    "            [light_value.append(y.text) for y in tds]          # getting list of values from each light row\n",
    "            Light_values.append(light_value)\n",
    "        if len(temps_dark)==(len(temps_light)+1) or len(temps_dark)==len(temps_light):\n",
    "            for i in range(len(temps_light)):                              \n",
    "                values.append(Dark_values[i])                   # list of values for months with even number of days\n",
    "                values.append(Light_values[i])                         \n",
    "        else:\n",
    "            for i in range(len(temps_light)):\n",
    "                values.append(Dark_values[i])                   # list of values for months with odd number of days\n",
    "                values.append(Light_values[i])\n",
    "            values.append(Dark_values[len(temps_light)])\n",
    "        tab={variables[i]:[x[i] for x in values] for i in range(len(variables))} # creating a monthly dictionary - variable:list_of_values\n",
    "        table2.append(tab)               # in the end, we get a list of monthly dictionaries\n",
    "        if index%200==0:     # checking the progress\n",
    "            print(index)\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(current_time)\n",
    "    except:\n",
    "        print(chosen_links2.index(link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## There were some errors for links 1268-1789. We will try to add them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "17:10:18\n",
      "1600\n",
      "17:11:46\n"
     ]
    }
   ],
   "source": [
    "ytable2=[]\n",
    "for link in chosen_links2[1268:1790]:\n",
    "    try:\n",
    "        index=chosen_links2.index(link)\n",
    "        month=final_months2[index]    # what month do we scrape?\n",
    "        year=str(final_years2[index])      # what year do we scrape?\n",
    "        city=final_cities2[index]    # what city do we scrape?\n",
    "        Dark_values=[]\n",
    "        Light_values=[]\n",
    "        values=[]\n",
    "        page = requests.get(link)             # getting linked webpage\n",
    "        soup1= Bsoup(page.text)\n",
    "        var=soup1.findAll(\"table\")[3].findAll(\"th\")    # find the list of variable names on the webpage\n",
    "        variables=[\"city\",\"year\",\"month\"]                    # making the list of variable names...\n",
    "        [variables.append(x.text) for x in var]     #  ... (for each link separately to check if they are always the same)\n",
    "        temps_dark = soup1.findAll(\"tr\",{'class':'dark'})    # data are in a table with alternating dark \n",
    "        temps_light = soup1.findAll(\"tr\",{'class':'light'})   # and light rows\n",
    "        for x in temps_dark:\n",
    "            tds=x.findAll(\"td\")\n",
    "            dark_value=[city,year,month]\n",
    "            [dark_value.append(y.text) for y in tds]            # getting list of values from each dark row\n",
    "            Dark_values.append(dark_value)\n",
    "        for x in temps_light:\n",
    "            tds=x.findAll(\"td\")\n",
    "            light_value=[city,year,month]\n",
    "            [light_value.append(y.text) for y in tds]          # getting list of values from each light row\n",
    "            Light_values.append(light_value)\n",
    "        if len(temps_dark)==(len(temps_light)+1) or len(temps_dark)==len(temps_light):\n",
    "            for i in range(len(temps_light)):                              \n",
    "                values.append(Dark_values[i])                   # list of values for months with even number of days\n",
    "                values.append(Light_values[i])                         \n",
    "        else:\n",
    "            for i in range(len(temps_light)):\n",
    "                values.append(Dark_values[i])                   # list of values for months with odd number of days\n",
    "                values.append(Light_values[i])\n",
    "            values.append(Dark_values[len(temps_light)])\n",
    "        tab={variables[i]:[x[i] for x in values] for i in range(len(variables))} # creating a monthly dictionary - variable:list_of_values\n",
    "        ytable2.append(tab)               # in the end, we get a list of monthly dictionaries\n",
    "        if index%200==0:     # checking the progress\n",
    "            print(index)\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(current_time)\n",
    "    except:\n",
    "        print(chosen_links2.index(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[table2.insert(1268+i,ytable2[i]) for i in range(len(ytable2))]\n",
    "#[table2.remove(x) for x in table2[988:988+len(ytable2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  Are the name of the variables always the same (including their order)?    '''\n",
    "sum([list(x.keys())==list(table2[0].keys()) for x in table2])==len(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    Creating list of dataframes - each month is a single dataframe  '''\n",
    "list_of_DFs2=[pd.DataFrame(x) for x in table2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''     Exporting dataframes to the csv files        '''\n",
    "df_paths2=['d:\\moje_dokumenty\\Desktop\\IES\\semester 11\\Python\\project\\data2\\ '+str(chosen_links2.index(x))+\".csv\" for x in chosen_links2]\n",
    "[list_of_DFs2[df_paths2.index(x)].to_csv(x, index = None, header=True) for x in df_paths2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do: \n",
    "## analysis of the data (in a new Jupyter notebook file) - first of all to process them: \n",
    "*  ##  delete %, km/h etc. from the values in dataframes +  to transform strings into numerical values (floats) for appropriate variables, eg \"6.6 °C\" (string) into 6.6 (float)\n",
    "* ## connected to the previous point, to replace missing values with monthly averages of the corresponding variable (or to drop the observation?)\n",
    "* ## adding new data column to dataframes: daily range of temperature as Tmax-Tmin\n",
    "* ## deleting unneccesary data columns from dataframes - Umidita, Raffica, Fenomeni,  Info (perhaps not really needed) \n",
    "* ## transliting Italian names of the variables (columns) into English, perhaps also name of the months inside of the dataframes\n",
    "* ## then we can compute quarterly(?) (ie January to March, April to June...) means and variances for each city\n",
    "\n",
    "## Regarding work done here, it will be needed to convert the code into objects and functions so it is object-oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous versions - garbage bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  basic links   '''\n",
    "link1=\"https://www.ilmeteo.it/portale/\"\n",
    "link2=\"https://www.ilmeteo.it/portale/archivio-meteo/\"\n",
    "\n",
    "''' webpage with the links to the cities' data'''  # we chooses cities based on what is shown on this page\n",
    "start = requests.get(\"https://www.ilmeteo.it/portale/archivio-meteo/Piemonte\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning = requests.get(link2) \n",
    "soup_beginning=Bsoup(beginning.text)\n",
    "horalka=soup_beginning.findAll(\"td\")\n",
    "rumba=horalka[1].findAll(\"a\")\n",
    "regions=[]\n",
    "for x in rumba:\n",
    "    regions.append(x.get(\"href\"))\n",
    "\n",
    "reg_links=[]\n",
    "for region in regions:\n",
    "    reg_links.append(link1+region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   Gerting links to the data pages for the cities  '''\n",
    "cities1=[]\n",
    "for region in reg_links:\n",
    "    reg_page = requests.get(region) \n",
    "    soup_reg_page=Bsoup(reg_page.text)\n",
    "    tea=soup_reg_page.findAll(\"div\",{\"class\":\"block noborder\"})\n",
    "    cofee=tea[0].findAll(\"a\",{\"target\":\"\"})\n",
    "    for x in cofee:\n",
    "        cities1.append(x.text)\n",
    "        \n",
    "cities1=list(set(cities1))    # to get rid of the reccurent cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Torino', 'Genova', 'Milano', 'Trento', 'Venezia', 'Trieste', 'Bologna', 'Firenze', 'Ancona', 'Perugia', \"L'Aquila\", 'Roma', 'Campobasso', 'Prato', 'Bari', 'Napoli', 'Potenza', 'Catanzaro', 'Palermo', 'Cagliari', 'Catania']\n"
     ]
    }
   ],
   "source": [
    "'''   Gerting links to the data pages for the cities  '''\n",
    "soup_start=Bsoup(start.text)\n",
    "\n",
    "cola=soup_start.findAll(\"tr\")\n",
    "\n",
    "pepsi=cola[0].findAll(\"a\")\n",
    "\n",
    "#links=[]\n",
    "#for city in pepsi:\n",
    "#    links.append(city.get(\"href\"))\n",
    "\n",
    "#links\n",
    "'''   Names of the cities '''\n",
    "cities=[]\n",
    "for city in pepsi:\n",
    "    cities.append(city.text)\n",
    "\n",
    "#print(links)\n",
    "print(cities)   # do we get what we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.ilmeteo.it/portale/archivio-meteo/Torino',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Genova',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Milano',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Trento',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Venezia',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Trieste',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Bologna',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Firenze',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Ancona',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Perugia',\n",
       " \"https://www.ilmeteo.it/portale/archivio-meteo/L'Aquila\",\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Roma',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Campobasso',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Prato',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Bari',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Napoli',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Potenza',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Catanzaro',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Palermo',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Cagliari',\n",
       " 'https://www.ilmeteo.it/portale/archivio-meteo/Catania']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''   Links to the cities' webpages  '''\n",
    "cities_data_pages=[]\n",
    "for city in cities:       # we actually do not need actual links from the previous step as the links are just combinations of \n",
    "    cities_data_pages.append(link2+city) # \"link2\" and the name of the city (+ links from previous step give grammatical errors)\n",
    "cities_data_pages   # do we get what we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for city_link in cities1_links:\n",
    "#    city_page = requests.get(city_link)\n",
    "#    page_text=Bsoup(city_page.text)  ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actaully, we can get the whole link by just combining the name of the city with the year, month and day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' List of years that we want to see  ''' \n",
    "years=[]\n",
    "for year in range(1985,2020):\n",
    "    years.append(str(year))\n",
    "\n",
    "    ''' Constructing the \"calendar\" as a dictionary  ''' \n",
    "month_names=[\"Gennaio\",\"Febbraio\",\"Marzo\",\"Aprile\",\"Maggio\",\"Giugno\",\"Luglio\",\"Agosto\",\"Settembre\",\"Ottobre\",\"Novembre\",\"Dicembre\"]\n",
    "zero=list(range(1,31))\n",
    "one=list(range(1,32))\n",
    "feb1=list(range(1,29))\n",
    "feb2=list(range(1,30))\n",
    "days=[one,zero,one,zero,one,zero,one,one,zero,one,zero,one]  \n",
    "##  allocating the correct number of days to each month (except February) \n",
    "calendar={month_names[i]:days[i] for i in range(12)}   \n",
    "\n",
    "'''  Construction of the final links to the wanted data tables  ''' \n",
    "table_links=[]\n",
    "for city_link in cities1_links:\n",
    "    for year in years:\n",
    "        if int(year)%4==0:\n",
    "            calendar[\"Febbraio\"]=feb2                # adjusting February days in a leap year\n",
    "            for month in list(calendar.keys()):\n",
    "                for day in calendar[month]:\n",
    "                    table_links.append(city_link+\"/\"+year+\"/\"+month+\"/\"+str(day))\n",
    "        else:\n",
    "            calendar[\"Febbraio\"]=feb1                # adjusting February days in a non-leap year\n",
    "            for month in list(calendar.keys()):\n",
    "                for day in calendar[month]:\n",
    "                    table_links.append(city_link+\"/\"+year+\"/\"+month+\"/\"+str(day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3 s ± 17.8 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%timeit\n",
    "final_values=[]\n",
    "final_variables=[]\n",
    "for table in table_links[15900:15900+200]:\n",
    "    page = requests.get(table)\n",
    "    page_soup= Bsoup(page.text) \n",
    "    temps_dark = page_soup.findAll(\"tr\",{'class':'dark'})     # data are inside of a dark-light table\n",
    "    temps_light = page_soup.findAll(\"tr\",{'class':'light'})\n",
    "    variables=[]\n",
    "    values=[]\n",
    "    for x in temps_dark:\n",
    "        tds=x.findAll(\"td\")                        \n",
    "        variables.append(tds[0].text)           # first column of the table gives variable name\n",
    "        values.append(tds[1].text)               # second column of the table gives the value\n",
    "    for y in temps_light:\n",
    "        tds=y.findAll(\"td\")\n",
    "        variables.append(tds[0].text)\n",
    "        values.append(tds[1].text)\n",
    "    final_values.append(values)\n",
    "    final_variables.append(variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357924"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' getting html (or what it is) of our webpage - Rome, June 2011 '''\n",
    "\n",
    "r = requests.get(\"https://www.ilmeteo.it/portale/archivio-meteo/Roma/2001/Maggio\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "soup1= Bsoup(r.text) \n",
    "temps_dark = soup1.findAll(\"tr\",class_=\"dark\")\n",
    "temps_light = soup1.findAll(\"tr\",{'class':'light'})\n",
    "table1=[(x.text,y.text) for x,y in list(zip(temps_dark,temps_light))]\n",
    "#table2=[x.text for x in temps_light]\n",
    "#table=list(zip(table1,table2))\n",
    "table=[]\n",
    "for i in range(len(temps_light)): \n",
    "    table.append(temps_dark[i].text)\n",
    "    table.append(temps_light[i].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['121 3   10   29    43  9 4              ',\n",
       " '222 6   13   30    45  16 5              ',\n",
       " '323 7   14   29    48  14 8              ',\n",
       " '423 6   12   30    42  11 1              ',\n",
       " '522 8   12   30    49  11 1              ',\n",
       " '623 3   11   31    50  14 8              ',\n",
       " '723 3   15   30    60  13              ',\n",
       " '820 3   13   24    57  22 2              ',\n",
       " '922 6   14   29    33  11 1              ',\n",
       " '1021 6   10   29    44  7 6              ',\n",
       " '1119 9   10   27      74  9 4       ',\n",
       " '1220 6   16   28      81  35 2     63      ',\n",
       " '1321 8   14   26    62  14 8              ',\n",
       " '1418 8   17   23      83  5 4       ',\n",
       " '1520   16   24      76  7 6       ',\n",
       " '1621 3   13   27    61  14 8              ',\n",
       " '1722 3   13   29    54  5 4              ',\n",
       " '1822 7   13   29    61  16 5              ',\n",
       " '1921 7   12   29    65  5 4              ',\n",
       " '2022 3   13   28    64  7 6       ',\n",
       " '2122 7   17   27    69  7 6              ',\n",
       " '2223 4   18   29    69  13              ',\n",
       " '2320 5   19   26      86  11 1       ',\n",
       " '2422 2   13   27    53  14 8              ',\n",
       " '2520 8   11   28    58  9 4              ',\n",
       " '2620 3   15   25      86  37       ',\n",
       " '2719 1   16   22      62  24 1       ',\n",
       " '2818 2   10   24    59  14 8              ',\n",
       " '2920 8   14   25      75  24 1       ',\n",
       " '3020 4   16   26      82  20 6       ',\n",
       " '118 6   14   23      72  13       ',\n",
       " '216 4   10   22    78  7 6              ',\n",
       " '317 5   8   23    59  14 8              ',\n",
       " '414 8   5   22    57  13              ',\n",
       " '5           ',\n",
       " '6           ',\n",
       " '7           ',\n",
       " '814 2   9   18    84  9 4              ',\n",
       " '914 4   6   21    73  9 4              ',\n",
       " '1017 3   8   24    71            ',\n",
       " '1118 8   9   24    68  14 8              ',\n",
       " '1218 7   17   21      80  35 2       ',\n",
       " '1314   12   17      92  7 6       ',\n",
       " '1415 7   9   20      87  14 8       ',\n",
       " '1516 1   13   18      95  11 1       ',\n",
       " '1616 3   13   20    86  9 4       ',\n",
       " '1714 9   11   20    82  9 4       ',\n",
       " '1817 9   15   21    71  42 4     59             ',\n",
       " '1914 1   12   15      91  13       ',\n",
       " '209 8   9   11      75  20 6       ',\n",
       " '2110 2   8   13    62  24 1     42             ',\n",
       " '228 2   3   12    66  14 8              ',\n",
       " '237 8   1 6   14    67  9 4              ',\n",
       " '247 9    1   15    62  3 5              ',\n",
       " '25           ',\n",
       " '268 3   0 6   16    73            ',\n",
       " '2712 1   9   13    65  7 6              ',\n",
       " '2811   9   14      91  9 4       ',\n",
       " '2910 7   9   12      79  11 1       ',\n",
       " '307 9   5   12    70  18 3              ']"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' getting rid of non-numerical text '''\n",
    "import re\n",
    "for i in range(len(table)):\n",
    "    table[i]=re.sub(\"[^0-9]\",\" \",table[i])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21 3   10   29    43  9 4              ',\n",
       " '22 6   13   30    45  16 5              ',\n",
       " '23 7   14   29    48  14 8              ',\n",
       " '23 6   12   30    42  11 1              ',\n",
       " '22 8   12   30    49  11 1              ',\n",
       " '23 3   11   31    50  14 8              ',\n",
       " '23 3   15   30    60  13              ',\n",
       " '20 3   13   24    57  22 2              ',\n",
       " '22 6   14   29    33  11 1              ',\n",
       " '21 6   10   29    44  7 6              ',\n",
       " '19 9   10   27      74  9 4       ',\n",
       " '20 6   16   28      81  35 2     63      ',\n",
       " '21 8   14   26    62  14 8              ',\n",
       " '18 8   17   23      83  5 4       ',\n",
       " '20   16   24      76  7 6       ',\n",
       " '21 3   13   27    61  14 8              ',\n",
       " '22 3   13   29    54  5 4              ',\n",
       " '22 7   13   29    61  16 5              ',\n",
       " '21 7   12   29    65  5 4              ',\n",
       " '22 3   13   28    64  7 6       ',\n",
       " '22 7   17   27    69  7 6              ',\n",
       " '23 4   18   29    69  13              ',\n",
       " '20 5   19   26      86  11 1       ',\n",
       " '22 2   13   27    53  14 8              ',\n",
       " '20 8   11   28    58  9 4              ',\n",
       " '20 3   15   25      86  37       ',\n",
       " '19 1   16   22      62  24 1       ',\n",
       " '18 2   10   24    59  14 8              ',\n",
       " '20 8   14   25      75  24 1       ',\n",
       " '20 4   16   26      82  20 6       ',\n",
       " '8 6   14   23      72  13       ',\n",
       " '6 4   10   22    78  7 6              ',\n",
       " '7 5   8   23    59  14 8              ',\n",
       " '4 8   5   22    57  13              ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '4 2   9   18    84  9 4              ',\n",
       " '4 4   6   21    73  9 4              ',\n",
       " '17 3   8   24    71            ',\n",
       " '18 8   9   24    68  14 8              ',\n",
       " '18 7   17   21      80  35 2       ',\n",
       " '14   12   17      92  7 6       ',\n",
       " '15 7   9   20      87  14 8       ',\n",
       " '16 1   13   18      95  11 1       ',\n",
       " '16 3   13   20    86  9 4       ',\n",
       " '14 9   11   20    82  9 4       ',\n",
       " '17 9   15   21    71  42 4     59             ',\n",
       " '14 1   12   15      91  13       ',\n",
       " '9 8   9   11      75  20 6       ',\n",
       " '10 2   8   13    62  24 1     42             ',\n",
       " '8 2   3   12    66  14 8              ',\n",
       " '7 8   1 6   14    67  9 4              ',\n",
       " '7 9    1   15    62  3 5              ',\n",
       " '           ',\n",
       " '8 3   0 6   16    73            ',\n",
       " '12 1   9   13    65  7 6              ',\n",
       " '11   9   14      91  9 4       ',\n",
       " '10 7   9   12      79  11 1       ',\n",
       " '7 9   5   12    70  18 3              ']"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' getting rid of the day of the month number so it does not mix up with median temperature '''\n",
    "\n",
    "# !! important!! -> do not repeat this code twice - if yes then we need to redefine table from the beginning (cell 3)\n",
    "\n",
    "for i in range(9):\n",
    "    table[i]=table[i][1:]\n",
    "for i in range(9,len(table)):\n",
    "    table[i]=table[i][2:]\n",
    "table    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' transforming table (which is truly a list of numbers and spaces) into a list of lists of individual values '''\n",
    "\n",
    "list_all=[]\n",
    "for row in table:\n",
    "    list_all.append(row.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' our values are still string characters, we want numbers '''\n",
    "for x in list_all:\n",
    "    for j in range(len(x)):\n",
    "        x[j]=float(x[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21.0, 3.0, 10.0, 29.0, 43.0, 9.0, 4.0],\n",
       " [22.0, 6.0, 13.0, 30.0, 45.0, 16.0, 5.0],\n",
       " [23.0, 7.0, 14.0, 29.0, 48.0, 14.0, 8.0],\n",
       " [23.0, 6.0, 12.0, 30.0, 42.0, 11.0, 1.0],\n",
       " [22.0, 8.0, 12.0, 30.0, 49.0, 11.0, 1.0],\n",
       " [23.0, 3.0, 11.0, 31.0, 50.0, 14.0, 8.0],\n",
       " [23.0, 3.0, 15.0, 30.0, 60.0, 13.0],\n",
       " [20.0, 3.0, 13.0, 24.0, 57.0, 22.0, 2.0],\n",
       " [22.0, 6.0, 14.0, 29.0, 33.0, 11.0, 1.0],\n",
       " [21.0, 6.0, 10.0, 29.0, 44.0, 7.0, 6.0],\n",
       " [19.0, 9.0, 10.0, 27.0, 74.0, 9.0, 4.0],\n",
       " [20.0, 6.0, 16.0, 28.0, 81.0, 35.0, 2.0, 63.0],\n",
       " [21.0, 8.0, 14.0, 26.0, 62.0, 14.0, 8.0],\n",
       " [18.0, 8.0, 17.0, 23.0, 83.0, 5.0, 4.0],\n",
       " [20.0, 16.0, 24.0, 76.0, 7.0, 6.0],\n",
       " [21.0, 3.0, 13.0, 27.0, 61.0, 14.0, 8.0],\n",
       " [22.0, 3.0, 13.0, 29.0, 54.0, 5.0, 4.0],\n",
       " [22.0, 7.0, 13.0, 29.0, 61.0, 16.0, 5.0],\n",
       " [21.0, 7.0, 12.0, 29.0, 65.0, 5.0, 4.0],\n",
       " [22.0, 3.0, 13.0, 28.0, 64.0, 7.0, 6.0],\n",
       " [22.0, 7.0, 17.0, 27.0, 69.0, 7.0, 6.0],\n",
       " [23.0, 4.0, 18.0, 29.0, 69.0, 13.0],\n",
       " [20.0, 5.0, 19.0, 26.0, 86.0, 11.0, 1.0],\n",
       " [22.0, 2.0, 13.0, 27.0, 53.0, 14.0, 8.0],\n",
       " [20.0, 8.0, 11.0, 28.0, 58.0, 9.0, 4.0],\n",
       " [20.0, 3.0, 15.0, 25.0, 86.0, 37.0],\n",
       " [19.0, 1.0, 16.0, 22.0, 62.0, 24.0, 1.0],\n",
       " [18.0, 2.0, 10.0, 24.0, 59.0, 14.0, 8.0],\n",
       " [20.0, 8.0, 14.0, 25.0, 75.0, 24.0, 1.0],\n",
       " [20.0, 4.0, 16.0, 26.0, 82.0, 20.0, 6.0],\n",
       " [8.0, 6.0, 14.0, 23.0, 72.0, 13.0],\n",
       " [6.0, 4.0, 10.0, 22.0, 78.0, 7.0, 6.0],\n",
       " [7.0, 5.0, 8.0, 23.0, 59.0, 14.0, 8.0],\n",
       " [4.0, 8.0, 5.0, 22.0, 57.0, 13.0],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [4.0, 2.0, 9.0, 18.0, 84.0, 9.0, 4.0],\n",
       " [4.0, 4.0, 6.0, 21.0, 73.0, 9.0, 4.0],\n",
       " [17.0, 3.0, 8.0, 24.0, 71.0],\n",
       " [18.0, 8.0, 9.0, 24.0, 68.0, 14.0, 8.0],\n",
       " [18.0, 7.0, 17.0, 21.0, 80.0, 35.0, 2.0],\n",
       " [14.0, 12.0, 17.0, 92.0, 7.0, 6.0],\n",
       " [15.0, 7.0, 9.0, 20.0, 87.0, 14.0, 8.0],\n",
       " [16.0, 1.0, 13.0, 18.0, 95.0, 11.0, 1.0],\n",
       " [16.0, 3.0, 13.0, 20.0, 86.0, 9.0, 4.0],\n",
       " [14.0, 9.0, 11.0, 20.0, 82.0, 9.0, 4.0],\n",
       " [17.0, 9.0, 15.0, 21.0, 71.0, 42.0, 4.0, 59.0],\n",
       " [14.0, 1.0, 12.0, 15.0, 91.0, 13.0],\n",
       " [9.0, 8.0, 9.0, 11.0, 75.0, 20.0, 6.0],\n",
       " [10.0, 2.0, 8.0, 13.0, 62.0, 24.0, 1.0, 42.0],\n",
       " [8.0, 2.0, 3.0, 12.0, 66.0, 14.0, 8.0],\n",
       " [7.0, 8.0, 1.0, 6.0, 14.0, 67.0, 9.0, 4.0],\n",
       " [7.0, 9.0, 1.0, 15.0, 62.0, 3.0, 5.0],\n",
       " [],\n",
       " [8.0, 3.0, 0.0, 6.0, 16.0, 73.0],\n",
       " [12.0, 1.0, 9.0, 13.0, 65.0, 7.0, 6.0],\n",
       " [11.0, 9.0, 14.0, 91.0, 9.0, 4.0],\n",
       " [10.0, 7.0, 9.0, 12.0, 79.0, 11.0, 1.0],\n",
       " [7.0, 9.0, 5.0, 12.0, 70.0, 18.0, 3.0]]"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
